"""
ITPF Legal Answer System - Hybrid AI-powered Q&A Endpoint for Vercel
Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ù„ÙŠ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ + ØªØ­Ù„ÙŠÙ„ DeepSeek Ø§Ù„Ø°ÙƒÙŠ
"""

import json
import os
import re
import requests
from typing import Dict, Any, List
import difflib
from http.server import BaseHTTPRequestHandler
from urllib.parse import parse_qs


def call_deepseek_api(prompt: str, max_tokens: int = 500) -> str:
    """Call DeepSeek API with intelligent prompt handling"""
    
    # Get API keys from environment (cycling through 3 keys for load balancing)
    api_keys = [
        os.getenv('DEEPSEEK_API_KEY_1'),
        os.getenv('DEEPSEEK_API_KEY_2'), 
        os.getenv('DEEPSEEK_API_KEY_3')
    ]
    
    # Filter out None values and placeholder values
    valid_keys = [key for key in api_keys if key and key != 'your-actual-deepseek-key-here' and key != 'your_actual_key_here']
    
    if not valid_keys:
        print("No valid DeepSeek API keys found - using algorithmic fallback")
        return None
    
    # Try each API key with intelligent rotation
    for i, api_key in enumerate(valid_keys):
        try:
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            # DeepSeek API endpoint (OpenAI compatible)
            url = "https://api.deepseek.com/v1/chat/completions"
            
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system",
                        "content": "Ø£Ù†Øª Ù‚Ø§Ø¶Ù ÙˆÙ…Ø­Ù„Ù„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§ØªØ­Ø§Ø¯ Ø§Ù„Ø¯ÙˆÙ„ÙŠ Ù„Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„Ø£ÙˆØªØ§Ø¯. ØªÙ‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…Ø®ØªØµØ±Ø© ÙˆØ°ÙƒÙŠØ© Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±ÙÙ‚Ø© ÙÙ‚Ø·."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": max_tokens,
                "temperature": 0.3,
                "stream": False
            }
            
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    ai_response = result['choices'][0]['message']['content'].strip()
                    return ai_response if ai_response else None
            else:
                print(f"DeepSeek API error {response.status_code}: {response.text}")
            
        except Exception as e:
            print(f"DeepSeek API error with key {i+1}: {str(e)}")
            continue
    
    # If all API keys failed, return None
    return None


def load_legal_data():
    """Load complete legal data from split JSON files"""
    try:
        # Get the directory of the current script (api directory)
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Load Arabic data from split files
        arabic_data = {"metadata": {}, "articles": [], "appendices": []}
        for i in range(1, 4):  # Load parts 1, 2, 3
            arabic_file = os.path.join(script_dir, f'arabic_data_part{i}.json')
            try:
                with open(arabic_file, 'r', encoding='utf-8') as f:
                    part_data = json.load(f)
                    if i == 1:  # Get metadata and appendices from first part
                        arabic_data["metadata"] = part_data.get("metadata", {})
                        arabic_data["appendices"] = part_data.get("appendices", [])
                    arabic_data["articles"].extend(part_data.get("articles", []))
            except Exception as e:
                print(f"Error loading Arabic part {i}: {str(e)}")
        
        print(f"Arabic data loaded: {len(arabic_data['articles'])} articles, {len(arabic_data['appendices'])} appendices")
        
        # Load English data from split files
        english_data = {"metadata": {}, "chapters": [], "appendices": []}
        for i in range(1, 4):  # Load parts 1, 2, 3
            english_file = os.path.join(script_dir, f'english_data_part{i}.json')
            try:
                with open(english_file, 'r', encoding='utf-8') as f:
                    part_data = json.load(f)
                    if i == 1:  # Get metadata and appendices from first part
                        english_data["metadata"] = part_data.get("metadata", {})
                        english_data["appendices"] = part_data.get("appendices", [])
                    english_data["chapters"].extend(part_data.get("chapters", []))
            except Exception as e:
                print(f"Error loading English part {i}: {str(e)}")
        
        # Extract articles from chapters for easier access
        all_english_articles = []
        if english_data["chapters"]:
            for chapter in english_data["chapters"]:
                all_english_articles.extend(chapter.get("articles", []))
        
        print(f"English data loaded: {len(all_english_articles)} articles, {len(english_data.get('appendices', []))} appendices")
        
        # Fallback: Try original files if split files not found
        if not arabic_data["articles"]:
            try:
                with open('arabic_legal_rules_complete_authentic.json', 'r', encoding='utf-8') as f:
                    arabic_data = json.load(f)
                    print(f"Fallback Arabic data loaded: {len(arabic_data.get('articles', []))} articles")
            except Exception as e:
                print(f"Error loading Arabic fallback data: {str(e)}")
        
        if not all_english_articles:
            try:
                with open('english_legal_rules_complete_authentic.json', 'r', encoding='utf-8') as f:
                    english_data = json.load(f)
                    print(f"Fallback English data loaded")
            except Exception as e:
                print(f"Error loading English fallback data: {str(e)}")
            
        return arabic_data, english_data
    except Exception as e:
        print(f"Critical error loading legal data: {str(e)}")
        return [], []


# PHASE 1: COMPREHENSIVE ITPF CONCEPT MAP AND SYNONYMS DICTIONARY
ITPF_CONCEPT_MAP = {
    "equipment": {
        "arabic_terms": ["Ø§Ù„Ù…Ø¹Ø¯Ø§Øª", "Ø§Ù„Ø£Ø¯ÙˆØ§Øª", "Ø§Ù„ØªØ¬Ù‡ÙŠØ²Ø§Øª", "Ø§Ù„Ø¹ØªØ§Ø¯", "Ø§Ù„Ø¢Ù„Ø§Øª", "Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©"],
        "english_terms": ["equipment", "gear", "tools", "apparatus", "devices", "instruments"],
        "synonyms": {
            "arabic": ["Ø§Ù„Ù„ÙˆØ§Ø²Ù…", "Ø§Ù„Ù…Ø³ØªÙ„Ø²Ù…Ø§Øª", "Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©", "Ø§Ù„ØªØ¬Ù‡ÙŠØ²Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©", "Ø§Ù„Ù…Ø¹Ø¯Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©"],
            "english": ["sporting goods", "athletic equipment", "necessary tools", "required apparatus", "contest gear"]
        },
        "related_concepts": ["safety", "specifications", "requirements", "approval", "certification"],
        "context_keywords": ["Ù…ÙˆØ§ØµÙØ§Øª", "Ù…ØªØ·Ù„Ø¨Ø§Øª", "Ø£Ù…Ø§Ù†", "Ø³Ù„Ø§Ù…Ø©", "Ø§Ø¹ØªÙ…Ø§Ø¯", "Ø´Ù‡Ø§Ø¯Ø©"]
    },
    "scoring": {
        "arabic_terms": ["Ø§Ù„Ù†Ù‚Ø§Ø·", "Ø§Ù„ØªØ³Ø¬ÙŠÙ„", "Ø§Ù„Ø¯Ø±Ø¬Ø§Øª", "Ø§Ù„Ø£Ø­Ø±Ø§Ø²", "Ø§Ù„Ù†ØªØ§Ø¦Ø¬", "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹"],
        "english_terms": ["scoring", "points", "grades", "marks", "tally", "count"],
        "synonyms": {
            "arabic": ["Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·", "Ø¬Ù…Ø¹ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø­Ø±Ø§Ø²", "Ø¥Ø­Ø±Ø§Ø² Ø§Ù„Ù†Ù‚Ø§Ø·", "Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"],
            "english": ["point counting", "score calculation", "result tallying", "grade computation", "final score"]
        },
        "related_concepts": ["competition", "judges", "rules", "results", "ranking"],
        "context_keywords": ["Ø­ÙƒÙ…", "ØªØ­ÙƒÙŠÙ…", "ØªØ±ØªÙŠØ¨", "Ù…Ù†Ø§ÙØ³Ø©", "ÙÙˆØ²", "ØªÙ‚ÙŠÙŠÙ…"]
    },
    "competition": {
        "arabic_terms": ["Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©", "Ø§Ù„Ø¨Ø·ÙˆÙ„Ø©", "Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©", "Ø§Ù„ØªÙ†Ø§ÙØ³", "Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø©", "Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬"],
        "english_terms": ["competition", "contest", "tournament", "championship", "match", "event"],
        "synonyms": {
            "arabic": ["Ø§Ù„ÙØ¹Ø§Ù„ÙŠØ©", "Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ", "Ø§Ù„Ù…Ù‡Ø±Ø¬Ø§Ù† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ", "Ø§Ù„Ø¯ÙˆØ±ÙŠ", "Ø§Ù„ÙƒØ£Ø³", "Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ"],
            "english": ["sporting event", "athletic contest", "competitive event", "tournament series", "championship series"]
        },
        "related_concepts": ["participants", "registration", "rules", "categories", "timing"],
        "context_keywords": ["Ù…Ø´Ø§Ø±Ùƒ", "Ù…Ø´Ø§Ø±ÙƒØ©", "ØªØ³Ø¬ÙŠÙ„", "ÙØ¦Ø©", "Ø²Ù…Ù†", "Ø¯ÙˆØ±", "Ù…Ø±Ø­Ù„Ø©"]
    },
    "safety": {
        "arabic_terms": ["Ø§Ù„Ø£Ù…Ø§Ù†", "Ø§Ù„Ø³Ù„Ø§Ù…Ø©", "Ø§Ù„Ø­Ù…Ø§ÙŠØ©", "Ø§Ù„ÙˆÙ‚Ø§ÙŠØ©", "Ø§Ù„Ø£Ù…Ù†", "Ø§Ù„ØªØ£Ù…ÙŠÙ†"],
        "english_terms": ["safety", "security", "protection", "precaution", "safeguarding"],
        "synonyms": {
            "arabic": ["Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ÙˆÙ‚Ø§Ø¦ÙŠØ©", "Ø§Ù„ØªØ¯Ø§Ø¨ÙŠØ± Ø§Ù„Ø£Ù…Ù†ÙŠØ©", "ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„Ø­Ù…Ø§ÙŠØ©", "Ø´Ø±ÙˆØ· Ø§Ù„Ø³Ù„Ø§Ù…Ø©"],
            "english": ["safety measures", "protective procedures", "security protocols", "safety requirements"]
        },
        "related_concepts": ["equipment", "rules", "supervision", "emergency", "medical"],
        "context_keywords": ["Ø·ÙˆØ§Ø±Ø¦", "Ø¥Ø³Ø¹Ø§Ù", "Ø·Ø¨ÙŠ", "Ø¥Ø´Ø±Ø§Ù", "Ù…Ø±Ø§Ù‚Ø¨Ø©", "Ø®Ø·Ø±"]
    },
    "judges": {
        "arabic_terms": ["Ø§Ù„Ø­ÙƒØ§Ù…", "Ø§Ù„Ù…Ø­ÙƒÙ…ÙŠÙ†", "Ø§Ù„Ù‚Ø¶Ø§Ø©", "Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨ÙŠÙ†", "Ø§Ù„Ù…Ø´Ø±ÙÙŠÙ†"],
        "english_terms": ["judges", "referees", "officials", "arbitrators", "supervisors"],
        "synonyms": {
            "arabic": ["Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ­ÙƒÙŠÙ…", "Ù„Ø¬Ù†Ø© Ø§Ù„Ø­ÙƒØ§Ù…", "Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠÙ†", "Ø§Ù„Ù‚Ø§Ø¦Ù…ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„ØªØ­ÙƒÙŠÙ…"],
            "english": ["judging panel", "officiating crew", "referee team", "judging committee"]
        },
        "related_concepts": ["scoring", "rules", "decisions", "appeals", "certification"],
        "context_keywords": ["Ù‚Ø±Ø§Ø±", "Ø­ÙƒÙ…", "ØªÙ‚ÙŠÙŠÙ…", "Ø§Ø¹ØªØ±Ø§Ø¶", "Ø§Ø³ØªØ¦Ù†Ø§Ù", "Ø´Ù‡Ø§Ø¯Ø©"]
    },
    "field": {
        "arabic_terms": ["Ø§Ù„Ù…ÙŠØ¯Ø§Ù†", "Ø§Ù„Ø³Ø§Ø­Ø©", "Ø§Ù„Ù…Ù†Ø·Ù‚Ø©", "Ø§Ù„Ù…Ù„Ø¹Ø¨", "Ø§Ù„Ø­Ù„Ø¨Ø©", "Ø§Ù„Ù…Ø¬Ø§Ù„"],
        "english_terms": ["field", "arena", "area", "ground", "venue", "pitch"],
        "synonyms": {
            "arabic": ["Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©", "Ù…ÙˆÙ‚Ø¹ Ø§Ù„ÙØ¹Ø§Ù„ÙŠØ©", "Ø£Ø±Ø¶ Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©", "Ø³Ø§Ø­Ø© Ø§Ù„Ù„Ø¹Ø¨"],
            "english": ["competition venue", "contest area", "playing field", "competition ground"]
        },
        "related_concepts": ["dimensions", "specifications", "preparation", "marking", "boundaries"],
        "context_keywords": ["Ø£Ø¨Ø¹Ø§Ø¯", "Ù…ÙˆØ§ØµÙØ§Øª", "ØªØ­Ø¶ÙŠØ±", "Ø¹Ù„Ø§Ù…Ø§Øª", "Ø­Ø¯ÙˆØ¯", "Ø®Ø·"]
    },
    "timing": {
        "arabic_terms": ["Ø§Ù„ØªÙˆÙ‚ÙŠØª", "Ø§Ù„Ø²Ù…Ù†", "Ø§Ù„Ù…Ø¯Ø©", "Ø§Ù„ÙˆÙ‚Øª", "Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ"],
        "english_terms": ["timing", "time", "duration", "period", "chronometer"],
        "synonyms": {
            "arabic": ["Ù‚ÙŠØ§Ø³ Ø§Ù„ÙˆÙ‚Øª", "Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¯Ø©", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø²Ù…Ù†Ø©", "Ø¶Ø¨Ø· Ø§Ù„ØªÙˆÙ‚ÙŠØª"],
            "english": ["time measurement", "duration recording", "chronometric recording", "time keeping"]
        },
        "related_concepts": ["competition", "results", "records", "precision", "equipment"],
        "context_keywords": ["Ø¯Ù‚Ø©", "Ø³Ø¬Ù„", "Ø±Ù‚Ù… Ù‚ÙŠØ§Ø³ÙŠ", "Ù‚ÙŠØ§Ø³", "Ø¶Ø¨Ø·"]
    },
    "registration": {
        "arabic_terms": ["Ø§Ù„ØªØ³Ø¬ÙŠÙ„", "Ø§Ù„ØªÙ‚ÙŠÙŠØ¯", "Ø§Ù„Ù‚ÙŠØ¯", "Ø§Ù„Ø§Ø´ØªØ±Ø§Ùƒ", "Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ©"],
        "english_terms": ["registration", "enrollment", "sign-up", "participation", "entry"],
        "synonyms": {
            "arabic": ["ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ©", "Ù‚ÙŠØ¯ Ø§Ù„Ø§Ø´ØªØ±Ø§Ùƒ", "Ø·Ù„Ø¨ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ©", "Ø§Ù„Ø§Ù†Ø¶Ù…Ø§Ù…"],
            "english": ["participant registration", "contest entry", "enrollment process", "sign-up procedure"]
        },
        "related_concepts": ["participants", "categories", "requirements", "deadlines", "fees"],
        "context_keywords": ["Ù…Ø´Ø§Ø±Ùƒ", "ÙØ¦Ø©", "Ù…ØªØ·Ù„Ø¨Ø§Øª", "Ù…ÙˆØ¹Ø¯ Ù†Ù‡Ø§Ø¦ÙŠ", "Ø±Ø³ÙˆÙ…"]
    },
    "rules": {
        "arabic_terms": ["Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯", "Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ†", "Ø§Ù„Ø£Ù†Ø¸Ù…Ø©", "Ø§Ù„Ù„ÙˆØ§Ø¦Ø­", "Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª"],
        "english_terms": ["rules", "regulations", "laws", "guidelines", "instructions"],
        "synonyms": {
            "arabic": ["Ø§Ù„Ø£Ø­ÙƒØ§Ù…", "Ø§Ù„Ø´Ø±ÙˆØ·", "Ø§Ù„Ø¶ÙˆØ§Ø¨Ø·", "Ø§Ù„Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ÙŠØ©", "Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©"],
            "english": ["provisions", "conditions", "controls", "guiding principles", "legal texts"]
        },
        "related_concepts": ["compliance", "violations", "penalties", "interpretation", "application"],
        "context_keywords": ["Ø§Ù„ØªØ²Ø§Ù…", "Ù…Ø®Ø§Ù„ÙØ©", "Ø¹Ù‚ÙˆØ¨Ø©", "ØªÙØ³ÙŠØ±", "ØªØ·Ø¨ÙŠÙ‚"]
    }
}

def expand_question_with_concepts(question: str) -> list:
    """
    Phase 1 Enhancement: Expand question using ITPF concept mapping and synonyms
    ØªÙÙƒÙŠÙƒ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ù„Ù‰ Ù…ÙØ§Ù‡ÙŠÙ… ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª ÙˆØ§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
    """
    question_lower = question.lower().strip()
    expanded_terms = []
    matched_concepts = []
    
    # Add original question terms
    original_terms = re.findall(r'\b[\u0600-\u06FFa-zA-Z]+\b', question_lower)
    expanded_terms.extend([term for term in original_terms if len(term) > 2])
    
    # Map question to ITPF concepts
    for concept_name, concept_data in ITPF_CONCEPT_MAP.items():
        concept_matched = False
        
        # Check Arabic terms
        for term in concept_data["arabic_terms"]:
            if term.lower() in question_lower:
                matched_concepts.append(concept_name)
                expanded_terms.extend(concept_data["arabic_terms"])
                expanded_terms.extend(concept_data["synonyms"]["arabic"])
                expanded_terms.extend(concept_data["context_keywords"])
                concept_matched = True
                break
        
        # Check English terms if not already matched
        if not concept_matched:
            for term in concept_data["english_terms"]:
                if term.lower() in question_lower:
                    matched_concepts.append(concept_name)
                    expanded_terms.extend(concept_data["english_terms"])
                    expanded_terms.extend(concept_data["synonyms"]["english"])
                    concept_matched = True
                    break
        
        # Check synonyms for partial matches
        if not concept_matched:
            for synonym in concept_data["synonyms"]["arabic"]:
                if any(word in question_lower for word in synonym.split()):
                    matched_concepts.append(concept_name)
                    expanded_terms.extend(concept_data["arabic_terms"])
                    expanded_terms.extend(concept_data["context_keywords"])
                    break
    
    # Add related concepts
    for matched_concept in matched_concepts:
        related_concepts = ITPF_CONCEPT_MAP[matched_concept].get("related_concepts", [])
        for related in related_concepts:
            if related in ITPF_CONCEPT_MAP:
                expanded_terms.extend(ITPF_CONCEPT_MAP[related]["arabic_terms"][:2])
                expanded_terms.extend(ITPF_CONCEPT_MAP[related]["english_terms"][:2])
    
    # Remove duplicates and filter
    unique_terms = list(set([term.lower() for term in expanded_terms if len(term) > 2]))
    
    print(f"Question expansion - Original: {len(original_terms)} terms, Expanded: {len(unique_terms)} terms")
    print(f"Matched concepts: {matched_concepts}")
    
    return unique_terms

# PHASE 2: ADVANCED CONTEXTUAL ANALYSIS SYSTEM
LEGAL_CONTEXT_HIERARCHIES = {
    "procedural": {
        "keywords": ["ÙƒÙŠÙ", "Ù…Ø§Ø°Ø§", "Ù…ØªÙ‰", "Ø£ÙŠÙ†", "how", "what", "when", "where", "procedure", "process"],
        "intent": "procedural_guidance",
        "priority_concepts": ["registration", "competition", "timing", "field"],
        "response_style": "step_by_step"
    },
    "regulatory": {
        "keywords": ["ÙŠØ¬Ø¨", "ÙŠÙÙ…Ù†Ø¹", "Ù…Ø·Ù„ÙˆØ¨", "Ø¶Ø±ÙˆØ±ÙŠ", "Ù‚Ø§Ù†ÙˆÙ†ÙŠ", "must", "required", "mandatory", "legal", "rule"],
        "intent": "rule_compliance", 
        "priority_concepts": ["rules", "safety", "equipment", "judges"],
        "response_style": "compliance_focused"
    },
    "technical": {
        "keywords": ["Ù…ÙˆØ§ØµÙØ§Øª", "ØªÙ‚Ù†ÙŠ", "Ù…Ø¹Ø¯Ø§Øª", "Ø£Ø¨Ø¹Ø§Ø¯", "Ù‚ÙŠØ§Ø³", "technical", "specifications", "measurements", "equipment"],
        "intent": "technical_details",
        "priority_concepts": ["equipment", "field", "timing", "specifications"],
        "response_style": "specification_detailed"
    },
    "competitive": {
        "keywords": ["Ù†Ù‚Ø§Ø·", "ÙÙˆØ²", "ØªØ±ØªÙŠØ¨", "Ø¨Ø·ÙˆÙ„Ø©", "Ù…Ø³Ø§Ø¨Ù‚Ø©", "points", "win", "competition", "tournament", "scoring"],
        "intent": "competition_mechanics",
        "priority_concepts": ["scoring", "competition", "judges", "rules"],
        "response_style": "outcome_focused"
    }
}

CONTEXTUAL_RELATIONSHIPS = {
    "equipment_safety": {
        "concepts": ["equipment", "safety"],
        "relationship": "Equipment must meet safety standards",
        "cross_references": ["approval", "certification", "specifications"]
    },
    "competition_scoring": {
        "concepts": ["competition", "scoring", "judges"],
        "relationship": "Competition results determined by judge scoring",
        "cross_references": ["rules", "rankings", "results"]
    },
    "field_timing": {
        "concepts": ["field", "timing", "competition"],
        "relationship": "Field specifications affect timing and competition flow",
        "cross_references": ["equipment", "safety", "specifications"]
    },
    "registration_participation": {
        "concepts": ["registration", "competition", "rules"],
        "relationship": "Registration requirements govern competition participation",
        "cross_references": ["categories", "requirements", "deadlines"]
    }
}

def analyze_question_context(question: str) -> dict:
    """
    Phase 2 Enhancement: Advanced contextual analysis of question intent
    ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„ÙÙ‡Ù… Ù†ÙŠØ© Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØªØµÙ†ÙŠÙÙ‡
    """
    question_lower = question.lower().strip()
    context_analysis = {
        "primary_intent": "general",
        "secondary_intents": [],
        "legal_hierarchy": "basic",
        "priority_concepts": [],
        "response_style": "balanced",
        "contextual_relationships": []
    }
    
    # Analyze question intent based on keywords
    intent_scores = {}
    for context_type, context_data in LEGAL_CONTEXT_HIERARCHIES.items():
        score = 0
        for keyword in context_data["keywords"]:
            if keyword in question_lower:
                score += 2
                # Check for question patterns that indicate specific intent
                if question_lower.startswith(("ÙƒÙŠÙ", "how", "Ù…Ø§Ø°Ø§", "what")):
                    if context_type == "procedural":
                        score += 3
                elif "ÙŠØ¬Ø¨" in question_lower or "must" in question_lower:
                    if context_type == "regulatory":
                        score += 3
                elif "Ù…ÙˆØ§ØµÙØ§Øª" in question_lower or "specifications" in question_lower:
                    if context_type == "technical":
                        score += 3
                elif "Ù†Ù‚Ø§Ø·" in question_lower or "points" in question_lower:
                    if context_type == "competitive":
                        score += 3
        intent_scores[context_type] = score
    
    # Determine primary intent
    if intent_scores:
        primary_intent = max(intent_scores.keys(), key=lambda k: intent_scores[k])
        if intent_scores[primary_intent] > 0:
            context_analysis["primary_intent"] = primary_intent
            context_analysis["priority_concepts"] = LEGAL_CONTEXT_HIERARCHIES[primary_intent]["priority_concepts"]
            context_analysis["response_style"] = LEGAL_CONTEXT_HIERARCHIES[primary_intent]["response_style"]
            
            # Add secondary intents (scores > 1)
            for intent, score in intent_scores.items():
                if intent != primary_intent and score > 1:
                    context_analysis["secondary_intents"].append(intent)
    
    # Identify relevant contextual relationships
    for rel_name, rel_data in CONTEXTUAL_RELATIONSHIPS.items():
        for concept in context_analysis["priority_concepts"]:
            if concept in rel_data["concepts"]:
                context_analysis["contextual_relationships"].append(rel_name)
                break
    
    print(f"Context analysis - Intent: {context_analysis['primary_intent']}, Style: {context_analysis['response_style']}")
    return context_analysis

def enhance_search_with_context(question: str, expanded_terms: list, context_analysis: dict) -> list:
    """
    Phase 2 Enhancement: Enhance search terms based on contextual analysis
    ØªØ¹Ø²ÙŠØ² Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ
    """
    enhanced_terms = expanded_terms.copy()
    
    # Add context-specific terms based on intent
    intent = context_analysis["primary_intent"]
    if intent in LEGAL_CONTEXT_HIERARCHIES:
        intent_data = LEGAL_CONTEXT_HIERARCHIES[intent]
        
        # Add priority concept terms
        for priority_concept in intent_data["priority_concepts"]:
            if priority_concept in ITPF_CONCEPT_MAP:
                concept_data = ITPF_CONCEPT_MAP[priority_concept]
                enhanced_terms.extend(concept_data["arabic_terms"][:3])
                enhanced_terms.extend(concept_data["english_terms"][:2])
                enhanced_terms.extend(concept_data["context_keywords"][:3])
    
    # Add terms from contextual relationships
    for rel_name in context_analysis["contextual_relationships"]:
        if rel_name in CONTEXTUAL_RELATIONSHIPS:
            rel_data = CONTEXTUAL_RELATIONSHIPS[rel_name]
            enhanced_terms.extend(rel_data["cross_references"])
    
    # Remove duplicates and return
    unique_enhanced_terms = list(set([term.lower() for term in enhanced_terms if len(term) > 2]))
    
    print(f"Enhanced search - Original: {len(expanded_terms)}, Enhanced: {len(unique_enhanced_terms)}")
    return unique_enhanced_terms

def advanced_search_relevant_content(question: str, data: list, language: str) -> list:
    """Advanced search algorithm enhanced with ITPF concept mapping and contextual analysis"""
    relevant_articles = []
    
    # PHASE 1: CONCEPT-ENHANCED SEARCH
    # Use concept mapping to expand search terms
    expanded_keywords = expand_question_with_concepts(question)
    
    # PHASE 2: CONTEXTUAL ANALYSIS ENHANCEMENT
    # Analyze question context and intent
    context_analysis = analyze_question_context(question)
    
    # Enhance search terms with contextual understanding
    enhanced_keywords = enhance_search_with_context(question, expanded_keywords, context_analysis)
    
    # Clean and prepare question for analysis
    question_lower = question.lower().strip()
    
    # Extract original keywords as backup
    arabic_keywords = re.findall(r'\b[\u0600-\u06FF]+\b', question_lower)
    english_keywords = re.findall(r'\b[a-zA-Z]+\b', question_lower)
    
    # Combine original, expanded, and contextually enhanced keywords
    all_keywords = list(set(enhanced_keywords + arabic_keywords + english_keywords))
    
    print(f"Search keywords extracted: {all_keywords}")
    
    for article in data:
        # Handle both dict and string formats
        if isinstance(article, dict):
            content_text = str(article.get('content', '')).lower()
            title_text = str(article.get('title', '')).lower()
            article_number = str(article.get('article_number', ''))
        else:
            # Skip invalid articles
            continue
        
        # Calculate relevance score
        relevance_score = 0
        matched_keywords = []
        
        # Direct keyword matching
        for keyword in all_keywords:
            if len(keyword) > 2:  # Ignore very short keywords
                if keyword in content_text:
                    relevance_score += 3
                    matched_keywords.append(keyword)
                elif keyword in title_text:
                    relevance_score += 2
                    matched_keywords.append(keyword)
        
        # Fuzzy matching for important terms
        important_terms = ['Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ©', 'Ø§Ù„ØªØ³Ø¬ÙŠÙ„', 'Ø§Ù„Ù†Ù‚Ø§Ø·', 'Ø§Ù„Ù…Ø¹Ø¯Ø§Øª', 'Ø§Ù„Ø¨Ø·ÙˆÙ„Ø©', 'Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯', 'Ø§Ù„Ù†Ø¸Ø§Ù…']
        for term in important_terms:
            if term in question_lower:
                # Check for similar terms in content
                similar_matches = difflib.get_close_matches(term, content_text.split(), n=3, cutoff=0.6)
                if similar_matches:
                    relevance_score += 1
                    matched_keywords.extend(similar_matches)
        
        # PHASE 2: CONTEXTUAL SCORING ENHANCEMENT
        # Apply contextual boosting based on intent analysis
        if relevance_score > 0:
            # Boost scores for articles that match the identified intent
            intent_boost = 0
            if context_analysis["primary_intent"] != "general":
                response_style = context_analysis["response_style"]
                
                # Boost based on response style requirements
                if response_style == "compliance_focused" and any(compliance_term in content_text for compliance_term in ["ÙŠØ¬Ø¨", "Ù…Ø·Ù„ÙˆØ¨", "Ø¶Ø±ÙˆØ±ÙŠ", "must", "required"]):
                    intent_boost += 2
                elif response_style == "specification_detailed" and any(spec_term in content_text for spec_term in ["Ù…ÙˆØ§ØµÙØ§Øª", "Ø£Ø¨Ø¹Ø§Ø¯", "Ù‚ÙŠØ§Ø³", "specifications", "measurements"]):
                    intent_boost += 2
                elif response_style == "step_by_step" and any(proc_term in content_text for proc_term in ["Ø®Ø·ÙˆØ§Øª", "ÙƒÙŠÙÙŠØ©", "Ø·Ø±ÙŠÙ‚Ø©", "steps", "procedure"]):
                    intent_boost += 2
                elif response_style == "outcome_focused" and any(outcome_term in content_text for outcome_term in ["Ù†ØªÙŠØ¬Ø©", "ÙÙˆØ²", "Ù†Ù‚Ø§Ø·", "result", "win", "points"]):
                    intent_boost += 2
            
            final_score = relevance_score + intent_boost
            
            article_copy = article.copy()
            article_copy['relevance_score'] = final_score
            article_copy['matched_keywords'] = matched_keywords
            article_copy['search_language'] = language
            article_copy['context_intent'] = context_analysis["primary_intent"]
            article_copy['intent_boost'] = intent_boost
            relevant_articles.append(article_copy)
    
    # Sort by relevance score (highest first)
    relevant_articles.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
    
    print(f"Found {len(relevant_articles)} relevant articles")
    return relevant_articles


def extract_search_keywords(question: str) -> List[str]:
    """Extract key search terms from complex questions"""
    import re
    
    # Arabic keywords mapping
    keyword_patterns = {
        'ØªØ£Ø®Ø±': ['ØªØ£Ø®ÙŠØ±', 'Ù…ØªØ£Ø®Ø±', 'ØªØ£Ø®Ø±', 'ÙˆÙ‚Øª'],
        'Ø¹Ù‚ÙˆØ¨Ø©': ['Ø¹Ù‚ÙˆØ¨Ø©', 'Ø¬Ø²Ø§Ø¡', 'Ø®ØµÙ…', 'Ø§Ø³ØªØ¨Ø¹Ø§Ø¯'],
        'Ø§Ø³ØªØ«Ù†Ø§Ø¡': ['Ø§Ø³ØªØ«Ù†Ø§Ø¡', 'Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª', 'Ø­Ø§Ù„Ø© Ø®Ø§ØµØ©', 'Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡'],
        'Ù†Ù‚Ø§Ø·': ['Ù†Ù‚Ø·Ø©', 'Ù†Ù‚Ø§Ø·', 'Ø¯Ø±Ø¬Ø©', 'Ø¯Ø±Ø¬Ø§Øª'],
        'ÙˆÙ‚Øª': ['Ø«Ø§Ù†ÙŠØ©', 'Ø¯Ù‚ÙŠÙ‚Ø©', 'ÙˆÙ‚Øª', 'Ø²Ù…Ù†'],
        'Ø´Ø§Ø±Ø©': ['Ø¥Ø´Ø§Ø±Ø©', 'Ø¨Ø¯Ø§ÙŠØ©', 'Ø§Ù†Ø·Ù„Ø§Ù‚', 'Ø´Ø§Ø±Ø©']
    }
    
    keywords = []
    question_lower = question.lower()
    
    # Extract direct keywords
    for main_keyword, variations in keyword_patterns.items():
        for variation in variations:
            if variation in question_lower:
                keywords.append(main_keyword)
                break
    
    # Extract numbers (time values)
    numbers = re.findall(r'\d+', question)
    if numbers:
        keywords.extend([f'{num}' for num in numbers])
    
    # Add the original question for comprehensive search
    keywords.append(question)
    
    return list(set(keywords))

def intelligent_question_decomposition(question: str, language: str) -> Dict[str, Any]:
    """
    Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: ØªÙÙƒÙŠÙƒ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙƒÙŠ Ø¥Ù„Ù‰ Ø¹Ù†Ø§ØµØ± ÙˆÙ…ÙØ§Ù‡ÙŠÙ… Ø£Ø³Ø§Ø³ÙŠØ©
    Phase 1: Intelligent question decomposition into basic elements and concepts
    """
    
    # ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØªØµÙ†ÙŠÙÙ‡
    question_analysis = {
        "original_question": question,
        "question_type": "general",  # definition, procedure, regulation, penalty, etc.
        "key_concepts": [],         # Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        "search_elements": [],      # Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
        "context_indicators": [],   # Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø³ÙŠØ§Ù‚
        "priority_areas": []        # Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø°Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
    }
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    question_lower = question.lower()
    
    # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
    if any(word in question_lower for word in ['Ù…Ø§ Ù‡ÙŠ', 'Ù…Ø§ Ù‡Ùˆ', 'what is', 'what are']):
        question_analysis["question_type"] = "definition"
    elif any(word in question_lower for word in ['ÙƒÙŠÙ', 'ÙƒÙŠÙÙŠØ©', 'how', 'Ø·Ø±ÙŠÙ‚Ø©']):
        question_analysis["question_type"] = "procedure"  
    elif any(word in question_lower for word in ['Ù‚ÙˆØ§Ø¹Ø¯', 'Ø´Ø±ÙˆØ·', 'Ù…ØªØ·Ù„Ø¨Ø§Øª', 'rules', 'requirements']):
        question_analysis["question_type"] = "regulation"
    elif any(word in question_lower for word in ['Ø¹Ù‚ÙˆØ¨Ø©', 'Ø¬Ø²Ø§Ø¡', 'penalty', 'punishment']):
        question_analysis["question_type"] = "penalty"
    elif any(word in question_lower for word in ['Ù…Ù„Ø­Ù‚', 'appendix', 'Ø¬Ø¯ÙˆÙ„']):
        question_analysis["question_type"] = "appendix"
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    key_terms = [
        'ØªÙ‚Ø§Ø· Ø§Ù„Ø£ÙˆØªØ§Ø¯', 'tent pegging', 'ÙØ§Ø±Ø³', 'rider', 'Ø­ØµØ§Ù†', 'horse',
        'Ù…Ø³Ø§Ø¨Ù‚Ø©', 'competition', 'Ø¨Ø·ÙˆÙ„Ø©', 'championship', 'Ù…ÙŠØ¯Ø§Ù†', 'arena',
        'Ø³Ø±Ø¹Ø©', 'speed', 'Ø¯Ù‚Ø©', 'accuracy', 'Ù†Ù‚Ø§Ø·', 'points', 'Ø¯Ø±Ø¬Ø§Øª', 'scores',
        'Ø­ÙƒÙ…', 'judge', 'ØªØ­ÙƒÙŠÙ…', 'judging', 'Ù„Ø¬Ù†Ø©', 'committee',
        'Ù…Ù„Ø­Ù‚', 'appendix', 'Ø¬Ø¯ÙˆÙ„', 'table', 'Ø±Ø³Ù…', 'diagram'
    ]
    
    for term in key_terms:
        if term in question_lower:
            question_analysis["key_concepts"].append(term)
    
    # ØªØ­Ø¯ÙŠØ¯ Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
    search_elements = question_analysis["key_concepts"].copy()
    
    # Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚
    if 'Ù…Ù„Ø­Ù‚' in question_lower or 'appendix' in question_lower:
        if '9' in question or 'ØªØ³Ø¹Ø©' in question_lower or 'nine' in question_lower:
            search_elements.extend(['Ù…Ù„Ø­Ù‚ 9', 'appendix 9', 'Ø£Ø´ÙˆØ§Ø·', 'rounds'])
        if '10' in question or 'Ø¹Ø´Ø±Ø©' in question_lower or 'ten' in question_lower:
            search_elements.extend(['Ù…Ù„Ø­Ù‚ 10', 'appendix 10', 'Ù…ÙŠØ¯Ø§Ù†', 'field'])
    
    question_analysis["search_elements"] = search_elements
    
    print(f"ØªÙ… ØªÙÙƒÙŠÙƒ Ø§Ù„Ø³Ø¤Ø§Ù„ - Ø§Ù„Ù†ÙˆØ¹: {question_analysis['question_type']}, Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: {question_analysis['key_concepts']}")
    return question_analysis

def extract_relevant_sentences(content: str, search_elements: List[str]) -> str:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø© Ù…Ù† Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¨Ø­Ø«
    Extract most relevant sentences from content based on search elements
    """
    
    if not content or not search_elements:
        return content[:200] + "..." if len(content) > 200 else content
    
    sentences = content.split('.')
    relevant_sentences = []
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¨Ø­Ø«
    for sentence in sentences:
        sentence_lower = sentence.lower().strip()
        if sentence_lower and len(sentence.strip()) > 10:  # ØªØ¬Ù†Ø¨ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
            # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØµÙ„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¨Ø­Ø«
            relevance_score = 0
            for element in search_elements:
                if element.lower() in sentence_lower:
                    relevance_score += 1
            
            if relevance_score > 0:
                relevant_sentences.append((sentence.strip(), relevance_score))
    
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¬Ù…Ù„ Ø­Ø³Ø¨ Ø§Ù„ØµÙ„Ø©
    relevant_sentences.sort(key=lambda x: x[1], reverse=True)
    
    # Ø£Ø®Ø° Ø£ÙØ¶Ù„ 3-5 Ø¬Ù…Ù„
    selected_sentences = [sent[0] for sent in relevant_sentences[:5]]
    
    if not selected_sentences:
        # Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ø¬Ù…Ù„ Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ Ø§Ø£Ø®Ø° Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙˆØ§Ù„Ø«Ø§Ù†ÙŠØ©
        selected_sentences = [sent.strip() for sent in sentences[:2] if sent.strip()]
    
    return '. '.join(selected_sentences) + '.'

def create_hybrid_intelligent_answer(question: str, relevant_articles: List[Dict[str, Any]], language: str) -> str:
    """
    Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°ÙƒÙŠØ© Ù…Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚
    Phase 2: Create intelligent summary with deep analysis using question decomposition
    """
    
    if not relevant_articles:
        return "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù…Ø·Ø§Ø¨Ù‚ Ù„Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©."
    
    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: ØªÙÙƒÙŠÙƒ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙƒÙŠ
    question_analysis = intelligent_question_decomposition(question, language)
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙÙƒÙŠÙƒ
    context_articles = relevant_articles[:3]
    context_text = ""
    specific_references = []
    
    for i, article in enumerate(context_articles, 1):
        article_num = article.get('article_number', f'Article {i}')
        title = article.get('title', 'Untitled')
        content = article.get('content', '')
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø© ÙÙ‚Ø· Ù…Ù† ÙƒÙ„ Ù…Ø§Ø¯Ø©
        relevant_content = extract_relevant_sentences(content, question_analysis["search_elements"])
        
        specific_references.append(f"Ø§Ù„Ù…Ø§Ø¯Ø© {article_num}")
        context_text += f"\n\n--- Ø§Ù„Ù…Ø§Ø¯Ø© {article_num}: {title} ---\n{relevant_content}"
    
    # Ø§Ù„Ø¨Ø±ÙˆÙ…ÙˆØª Ø§Ù„Ù…Ø­Ø³Ù† Ù„Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°ÙƒÙŠØ© Ù…Ø¹ Ø§Ù„ØªÙÙƒÙŠÙƒ
    expert_prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§ØªØ­Ø§Ø¯ Ø§Ù„Ø¯ÙˆÙ„ÙŠ Ù„Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„Ø£ÙˆØªØ§Ø¯.

ØªÙ… ØªÙÙƒÙŠÙƒ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø°ÙƒÙŠØ§Ù‹ Ø¥Ù„Ù‰:
- Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„: {question_analysis["question_type"]}  
- Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {', '.join(question_analysis["key_concepts"])}
- Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¨Ø­Ø«: {', '.join(question_analysis["search_elements"])}

Ø§Ù„Ù…Ù‡Ù…Ø©: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙˆØªÙ‚Ø¯ÙŠÙ… Ø®Ù„Ø§ØµØ© Ø°ÙƒÙŠØ© Ù…Ø¹ Ø§Ù„Ø§Ø³ØªÙ†Ø§Ø¯ Ø§Ù„Ù…Ø­Ø¯Ø¯.

ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°ÙƒÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:

ğŸ§  **Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°ÙƒÙŠØ©:**

[Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø© ÙˆÙˆØ§Ø¶Ø­Ø© Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ ØªÙÙƒÙŠÙƒ Ø§Ù„Ø³Ø¤Ø§Ù„]

**Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ‚:**
Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„Ø£ÙˆØªØ§Ø¯ØŒ [ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ ÙˆØ´Ø§Ù…Ù„ ÙŠØ±Ø¨Ø· Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ù…Ù†Ø·Ù‚ÙŠØ§Ù‹ ÙˆÙŠÙˆØ¶Ø­ Ø§Ù„Ø­ÙƒÙ…Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ±Ø§Ø¡Ù‡Ø§ - ÙÙ‚Ø±Ø© Ù…ÙØµÙ„Ø© ØªÙØ¸Ù‡Ø± Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ø³ÙŠØ§Ù‚].

**Ø§Ù„Ø£Ø³Ø³ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©:**
â€¢ Ø§Ø³ØªÙ†Ø§Ø¯Ø§Ù‹ Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø±Ù‚Ù… [X]: "[Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ù†ÙŠ Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙ‚Ø· - ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ø§Ø¯Ø© ÙƒØ§Ù…Ù„Ø©]"
â€¢ Ø§Ø³ØªÙ†Ø§Ø¯Ø§Ù‹ Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø±Ù‚Ù… [Y]: "[Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ù†ÙŠ Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙ‚Ø·]"
[Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©]

**Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:**
[ÙÙ‚Ø±Ø© ØªØ­Ù„ÙŠÙ„ÙŠØ© Ø¹Ù…ÙŠÙ‚Ø© ØªØ±Ø¨Ø· Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© Ù…Ù†Ø·Ù‚ÙŠØ§Ù‹ ÙˆØªÙˆØ¶Ø­ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„ÙŠ ÙˆØ§Ù„Ø­ÙƒÙ…Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©]

Ù…ØªØ·Ù„Ø¨Ø§Øª ØµØ§Ø±Ù…Ø©:
1. Ø§Ø¨Ø¯Ø£ Ø¨Ù€ "ğŸ§  **Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°ÙƒÙŠØ©:**"
2. Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø¹Ù…ÙŠÙ‚Ø§Ù‹ Ù…ÙØµÙ„Ø§Ù‹  
3. Ø§Ø±Ø¨Ø· Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ù†Ø·Ù‚ÙŠØ§Ù‹ ÙˆØ°ÙƒÙŠØ§Ù‹
4. Ø§Ø°ÙƒØ± Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯ Ø¨ÙˆØ¶ÙˆØ­
5. Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±ÙÙ‚Ø©
6. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø§Ù„Ù…Ø¹Ù†ÙŠØ© ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…ÙˆØ§Ø¯ ÙƒØ§Ù…Ù„Ø©

Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ: {question}

Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {context_text}

Ù‚Ø¯Ù… Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°ÙƒÙŠØ© Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:"""

    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ù…Ù† DeepSeek
    ai_response = call_deepseek_api(expert_prompt, max_tokens=400)
    
    if ai_response:
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°ÙƒÙŠØ© ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        if "ğŸ§ " not in ai_response:
            ai_response = f"ğŸ§  **Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°ÙƒÙŠØ©:**\n\n{ai_response}"
        
        return ai_response
    else:
        # Fallback Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠ Ø§Ù„Ø°ÙƒÙŠ
        print("AI response failed, using intelligent algorithmic fallback")
        return create_smart_algorithmic_fallback(question, question_analysis, context_articles)


def create_smart_algorithmic_fallback(question: str, question_analysis: Dict[str, Any], articles: List[Dict[str, Any]]) -> str:
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¬Ø§Ø¨Ø© Ø°ÙƒÙŠØ© Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø¹Ù†Ø¯ Ø¹Ø¯Ù… ØªÙˆÙØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
    Create smart algorithmic fallback response when AI is unavailable
    """
    
    if not articles:
        return "ğŸ§  **Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°ÙƒÙŠØ©:**\n\nÙ„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù…Ø·Ø§Ø¨Ù‚ Ù„Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©."
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø©
    main_article = articles[0]
    article_num = main_article.get('article_number', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
    title = main_article.get('title', '')
    content = main_article.get('content', '')
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø©
    relevant_content = extract_relevant_sentences(content, question_analysis["search_elements"])
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ©
    smart_response = f"""ğŸ§  **Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°ÙƒÙŠØ©:**

{relevant_content}

**Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ‚:**
Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„Ø£ÙˆØªØ§Ø¯ØŒ ÙŠØªØ¶Ø­ Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ù† Ù†ÙˆØ¹ "{question_analysis['question_type']}" ÙˆÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {', '.join(question_analysis['key_concepts']) if question_analysis['key_concepts'] else 'Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø¹Ø§Ù…Ø©'}. 

**Ø§Ù„Ø£Ø³Ø³ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©:**
â€¢ Ø§Ø³ØªÙ†Ø§Ø¯Ø§Ù‹ Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø±Ù‚Ù… {article_num}: "{relevant_content[:150]}..."

**Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:**
Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙŠÙˆØ¶Ø­ Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…Ø¹Ù†ÙŠ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø·Ø±ÙˆØ­ØŒ ÙˆÙŠÙ‚Ø¯Ù… Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ù„Ø§Ø²Ù… Ù„Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„ÙŠ ÙˆÙÙ‚Ø§Ù‹ Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§ØªØ­Ø§Ø¯ Ø§Ù„Ø¯ÙˆÙ„ÙŠ Ù„Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„Ø£ÙˆØªØ§Ø¯."""

    # Ø¥Ø¶Ø§ÙØ© Ù…Ø±Ø§Ø¬Ø¹ Ø¥Ø¶Ø§ÙÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªÙˆÙØ±Ø©
    if len(articles) > 1:
        additional_refs = [f"Ø§Ù„Ù…Ø§Ø¯Ø© {art.get('article_number', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}" for art in articles[1:3]]
        smart_response += f"\n\n*Ù…Ø±Ø§Ø¬Ø¹ Ø¥Ø¶Ø§ÙÙŠØ©: {', '.join(additional_refs)}*"
    
    return smart_response

def create_intelligent_algorithmic_fallback(question: str, articles: List[Dict[str, Any]]) -> str:
    """Create intelligent fallback response when AI is unavailable"""
    
    if not articles:
        return "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù…Ø·Ø§Ø¨Ù‚ Ù„Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©."
    
    # Extract key information from the most relevant article
    main_article = articles[0]
    article_num = main_article.get('article_number', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
    title = main_article.get('title', '')
    content = main_article.get('content', '')
    
    # Extract relevant sentence from content based on question
    question_lower = question.lower()
    sentences = content.split('.')
    relevant_sentence = ""
    
    # Find most relevant sentence
    for sentence in sentences:
        sentence_lower = sentence.lower().strip()
        if any(keyword in sentence_lower for keyword in ['ÙŠØ¬Ø¨', 'ØªÙƒÙˆÙ†', 'ÙŠØªÙ…', 'Ø§Ù„Ù…Ø·Ù„ÙˆØ¨', 'Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠ']):
            if len(sentence.strip()) > 20:  # Avoid very short sentences
                relevant_sentence = sentence.strip()
                break
    
    if not relevant_sentence and sentences:
        relevant_sentence = sentences[0].strip()  # Fallback to first sentence
    
    # Create intelligent response
    if relevant_sentence:
        response = f"{relevant_sentence}. (Ø§Ù„Ù…Ø±Ø¬Ø¹: Ø§Ù„Ù…Ø§Ø¯Ø© {article_num})"
    else:
        response = f"ÙˆÙÙ‚Ø§Ù‹ Ù„Ù„Ù…Ø§Ø¯Ø© {article_num}: {title}ØŒ ÙŠØ±Ø¬Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©."
    
    # Add additional references if available
    if len(articles) > 1:
        additional_refs = [f"Ø§Ù„Ù…Ø§Ø¯Ø© {art.get('article_number', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}" for art in articles[1:3]]
        response += f" Ù…ÙˆØ§Ø¯ Ø°Ø§Øª ØµÙ„Ø©: {', '.join(additional_refs)}"
    
    return response


def create_hybrid_supporting_articles(articles: List[Dict[str, Any]], question: str) -> List[Dict[str, Any]]:
    """Create intelligent summaries for supporting articles"""
    
    supporting_articles = []
    
    for article in articles[:5]:  # Limit to top 5 supporting articles
        article_num = article.get('article_number', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
        title = article.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')
        content = article.get('content', '')
        relevance_score = article.get('relevance_score', 0)
        
        # Create AI-powered summary
        summary_prompt = f"""Ù„Ø®Øµ Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙÙŠ 2-3 Ø¬Ù…Ù„ ÙÙ‚Ø· Ø¨Ù…Ø§ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø·Ø±ÙˆØ­:

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}
Ø§Ù„Ù†Øµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ - Ø§Ù„Ù…Ø§Ø¯Ø© {article_num}: {title}
Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {content[:800]}...

Ù‚Ø¯Ù… Ù…Ù„Ø®Øµ Ù…ÙÙŠØ¯ ÙˆÙ…Ø®ØªØµØ± Ù„Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:"""
        
        ai_summary = call_deepseek_api(summary_prompt, max_tokens=100)
        
        if ai_summary:
            summary_content = ai_summary
        else:
            # Intelligent algorithmic summary
            summary_content = f"{title}. {content[:200]}..."
        
        supporting_article = {
            "title": f"Ø§Ù„Ù…Ø§Ø¯Ø© {article_num}: {title}",
            "content": summary_content,
            "article_number": article_num,
            "relevance_score": relevance_score,
            "ai_processed": bool(ai_summary)
        }
        
        supporting_articles.append(supporting_article)
    
    return supporting_articles


class handler(BaseHTTPRequestHandler):
    """
    Vercel serverless function handler for ITPF Legal Answer System
    Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ù„ÙŠ + ØªØ­Ù„ÙŠÙ„ DeepSeek AI
    """
    
    def _set_cors_headers(self):
        """Set CORS headers for all responses"""
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.send_header('Content-Type', 'application/json; charset=utf-8')
    
    def _send_json_response(self, status_code: int, data: dict):
        """Send JSON response with proper headers and cache busting"""
        self.send_response(status_code)
        self._set_cors_headers()
        # Force cache busting
        self.send_header('Cache-Control', 'no-cache, no-store, must-revalidate')
        self.send_header('Pragma', 'no-cache')
        self.send_header('Expires', '0')
        self.send_header('X-Content-Version', '4.0.0')
        self.end_headers()
        json_response = json.dumps(data, ensure_ascii=False).encode('utf-8')
        self.wfile.write(json_response)
    
    def do_OPTIONS(self):
        """Handle CORS preflight requests"""
        self.send_response(200)
        self._set_cors_headers()
        self.end_headers()
    
    def do_GET(self):
        """Handle GET request - API info"""
        api_info = {
            "message": "ITPF Legal Answer System - Hybrid AI",
            "version": "4.1.0",
            "status": "active",
            "system_type": "Hybrid: Local Search + DeepSeek AI Analysis",
            "endpoint": "/api/answer",
            "method": "POST",
            "usage": {
                "question": "Your legal question here",
                "language": "both|arabic|english"
            },
            "features": [
                "Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ",
                "Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ù„ÙŠ Ø§Ù„Ø¯Ù‚ÙŠÙ‚",
                "ØªØ­Ù„ÙŠÙ„ DeepSeek AI Ø§Ù„Ø¹Ù…ÙŠÙ‚", 
                "Ù…Ù†Ø¹ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù‚ ÙˆØ§Ù„Ù…Ø±Ø§ÙˆØºØ©",
                "Ù…Ø±Ø§Ø¬Ø¹ Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯",
                "Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ÙƒØ§Ù…Ù„Ø©",
                "Ø¯Ø¹Ù… Ø§Ù„Ù„ØºØªÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©",
                "55 Ù…Ø§Ø¯Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© + Ù…Ù„Ø­Ù‚ÙŠÙ† 9 Ùˆ 10"
            ],
            "data_integrity": {
                "arabic_articles": "55 + appendices 9,10",
                "english_articles": "55 + appendices 9,10", 
                "text_preservation": "Complete - no truncation",
                "last_verified": "2025-01-08T14:00:00"
            }
        }
        self._send_json_response(200, api_info)
    
    def do_POST(self):
        """Handle POST request - Question answering"""
        try:
            # Read request body
            content_length = int(self.headers.get('Content-Length', 0))
            post_data = self.rfile.read(content_length).decode('utf-8')
            
            # Parse JSON data
            try:
                data = json.loads(post_data)
            except json.JSONDecodeError:
                self._send_json_response(400, {
                    "success": False,
                    "message": "Invalid JSON in request body"
                })
                return
            
            question = data.get('question', '').strip()
            language = data.get('language', 'both')
            
            if not question:
                self._send_json_response(400, {
                    "success": False,
                    "message": "Question is required"
                })
                return
            
            print(f"Processing question: {question} (language: {language})")
            
            # Load complete legal data
            arabic_data, english_data = load_legal_data()
            
            if not arabic_data and not english_data:
                self._send_json_response(500, {
                    "success": False,
                    "message": "Legal database unavailable"
                })
                return
            
            # Phase 1: Fast Direct Search - Single query for speed
            relevant_articles = []
            if language in ['both', 'arabic'] and arabic_data:
                # Extract articles and appendices from Arabic data structure
                if isinstance(arabic_data, dict):
                    arabic_articles = arabic_data.get('articles', [])
                    arabic_appendices = arabic_data.get('appendices', [])
                    # Combine articles and appendices for comprehensive search
                    arabic_content = arabic_articles + arabic_appendices
                else:
                    arabic_content = arabic_data
                
                # Single fast search with original question
                arabic_results = advanced_search_relevant_content(question, arabic_content, 'arabic')
                relevant_articles.extend(arabic_results)
                
            if language in ['both', 'english'] and english_data:
                # Extract articles and appendices from English data structure 
                english_content = []
                if isinstance(english_data, dict):
                    if 'chapters' in english_data:
                        for chapter in english_data['chapters']:
                            english_content.extend(chapter.get('articles', []))
                    # Also include appendices if available
                    english_appendices = english_data.get('appendices', [])
                    english_content.extend(english_appendices)
                elif isinstance(english_data, list):
                    english_content = english_data
                english_results = advanced_search_relevant_content(question, english_content, 'english')
                relevant_articles.extend(english_results)
            
            # Sort all results by relevance
            relevant_articles.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
            
            if not relevant_articles:
                self._send_json_response(200, {
                    "success": True,
                    "legal_analysis": "ğŸ§  **Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°ÙƒÙŠØ©:**\n\nÙ„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù…Ø·Ø§Ø¨Ù‚ Ù„Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©. ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ø®ØªÙ„ÙØ©.",
                    "legal_references": [],
                    "metadata": {
                        "question": question,
                        "language": language,
                        "articles_found": 0,
                        "system_type": "Hybrid AI",
                        "search_status": "No relevant content found"
                    }
                })
                return
            
            # Phase 2: AI Analysis - DeepSeek analyzes found articles
            direct_answer = create_hybrid_intelligent_answer(question, relevant_articles, language)
            
            # Phase 3: Create supporting articles with AI summaries
            supporting_articles = create_hybrid_supporting_articles(relevant_articles, question)
            
            # Prepare final response with new format
            api_response = {
                "success": True,
                "legal_analysis": direct_answer,
                "legal_references": supporting_articles,
                "metadata": {
                    "question": question,
                    "language": language,
                    "articles_found": len(relevant_articles),
                    "system_type": "Hybrid: Local Search + DeepSeek AI",
                    "ai_powered": True,
                    "text_preservation": "Complete - no truncation",
                    "version": "4.1.0",
                    "cache_version": "21.0",
                    "timestamp": "2025-01-10T12:00:00",
                    "data_sources": {
                        "arabic_articles": len(arabic_data) if arabic_data else 0,
                        "english_articles": len(english_data) if english_data else 0
                    }
                }
            }
            
            self._send_json_response(200, api_response)
            
        except Exception as e:
            import traceback
            print(f"Error processing question: {str(e)}")
            print(f"Full traceback: {traceback.format_exc()}")
            self._send_json_response(500, {
                "success": False,
                "message": f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}",
                "error_details": str(e),
                "system_type": "Hybrid AI System"
            })