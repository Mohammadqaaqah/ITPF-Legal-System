"""
ITPF Advanced Legal Reasoning System
Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø£Ø­Ø¯Ø« ØªÙ‚Ù†ÙŠØ§Øª 2025

ÙŠØ­Ø§ÙƒÙŠ Ø·Ø±ÙŠÙ‚Ø© ØªÙÙƒÙŠØ± Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠÙŠÙ† ÙÙŠ Ø±Ø¨Ø· Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬
"""

import json
import re
from typing import Dict, Any, List, Tuple, Set, Optional
from dataclasses import dataclass
from collections import defaultdict
import asyncio
from datetime import datetime


@dataclass
class LegalEntity:
    """ÙƒÙŠØ§Ù† Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ù† Ø§Ù„Ù†Øµ"""
    text: str
    entity_type: str  # time, penalty, procedure, exception
    value: Optional[str] = None
    context: str = ""
    article_refs: List[int] = None


@dataclass
class CrossReference:
    """Ù…Ø±Ø¬Ø¹ Ù…ØªÙ‚Ø§Ø·Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©"""
    source_article: int
    target_article: int
    relationship_type: str  # defines, modifies, excepts, requires
    strength: float
    context: str


@dataclass  
class ReasoningStep:
    """Ø®Ø·ÙˆØ© ÙÙŠ Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ"""
    step_number: int
    reasoning: str
    evidence: str
    article_refs: List[int]
    confidence: float


class AdvancedLegalReasoning:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self):
        self.knowledge_graph = defaultdict(list)
        self.legal_entities = {}
        self.cross_references = []
        self.semantic_clusters = {}
        
        # Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
        self.relationship_patterns = {
            'defines': [
                r'ÙŠÙØ¹Ø±Ù.*?Ø¨Ø£Ù†Ù‡', r'ÙŠÙÙ‚ØµØ¯ Ø¨Ù€', r'Ù…ØµØ·Ù„Ø­.*?ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰',
                r'defined as', r'means', r'refers to'
            ],
            'modifies': [
                r'Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡', r'Ø¥Ù„Ø§ Ø£Ù†', r'ÙˆÙ„ÙƒÙ†', r'ØºÙŠØ± Ø£Ù†',
                r'except', r'unless', r'however', r'but'
            ],
            'requires': [
                r'ÙŠØ¬Ø¨', r'ÙŠØªÙˆØ¬Ø¨', r'ÙŠÙØ´ØªØ±Ø·', r'Ù„Ø§ Ø¨Ø¯ Ù…Ù†',
                r'must', r'shall', r'required', r'mandatory'
            ],
            'excepts': [
                r'ÙÙŠ Ø­Ø§Ù„Ø©', r'Ø¥Ø°Ø§ ÙƒØ§Ù†', r'Ø¹Ù†Ø¯ Ø­Ø¯ÙˆØ«', r'Ø§Ø³ØªØ«Ù†Ø§Ø¡',
                r'in case of', r'if', r'when', r'exception'
            ]
        }
        
        # ÙƒÙŠØ§Ù†Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ù‡Ù…Ø©
        self.entity_patterns = {
            'time': [
                r'(\d+)\s*(?:Ø«Ø§Ù†ÙŠØ©|Ø¯Ù‚ÙŠÙ‚Ø©|Ø³Ø§Ø¹Ø©|ÙŠÙˆÙ…)',
                r'(\d+)\s*(?:second|minute|hour|day)s?'
            ],
            'penalty': [
                r'ØµÙØ±\s*Ù†Ù‚Ø§Ø·?', r'Ø§Ø³ØªØ¨Ø¹Ø§Ø¯', r'Ø®ØµÙ…', r'Ø¹Ù‚ÙˆØ¨Ø©',
                r'zero\s*points?', r'disqualification', r'penalty'
            ],
            'score': [
                r'(\d+)\s*Ù†Ù‚Ø§Ø·?', r'(\d+)\s*points?'
            ]
        }

    def build_knowledge_graph(self, legal_data: Dict[str, Any]) -> None:
        """Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©"""
        print("ðŸ§  Building advanced knowledge graph...")
        
        all_articles = legal_data.get('articles', []) + legal_data.get('appendices', [])
        
        for article in all_articles:
            article_num = article.get('article_number', 'appendix')
            content = article.get('content', '')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
            entities = self._extract_legal_entities(content, article_num)
            self.legal_entities[article_num] = entities
            
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹Ø©
            cross_refs = self._find_cross_references(article, all_articles)
            self.cross_references.extend(cross_refs)
            
            # ØªØ¬Ù…ÙŠØ¹ Ø¯Ù„Ø§Ù„ÙŠ Ù„Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…ØªØ±Ø§Ø¨Ø·Ø©
            self._build_semantic_clusters(article, all_articles)

    def _extract_legal_entities(self, content: str, article_num) -> List[LegalEntity]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ"""
        entities = []
        
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, content, re.IGNORECASE)
                for match in matches:
                    entity = LegalEntity(
                        text=match.group(0),
                        entity_type=entity_type,
                        value=match.group(1) if match.groups() else match.group(0),
                        context=content[max(0, match.start()-50):match.end()+50],
                        article_refs=[article_num]
                    )
                    entities.append(entity)
        
        return entities

    def _find_cross_references(self, article: Dict, all_articles: List[Dict]) -> List[CrossReference]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…ÙˆØ§Ø¯"""
        cross_refs = []
        source_num = article.get('article_number', 'appendix')
        content = article.get('content', '').lower()
        
        for rel_type, patterns in self.relationship_patterns.items():
            for pattern in patterns:
                if re.search(pattern, content):
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©
                    for other_article in all_articles:
                        target_num = other_article.get('article_number', 'appendix')
                        if target_num != source_num:
                            # Ø­Ø³Ø§Ø¨ Ù‚ÙˆØ© Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
                            strength = self._calculate_semantic_similarity(
                                content, other_article.get('content', '').lower()
                            )
                            
                            if strength > 0.3:  # Ø¹ØªØ¨Ø© Ø§Ù„Ø¹Ù„Ø§Ù‚Ø©
                                cross_ref = CrossReference(
                                    source_article=source_num,
                                    target_article=target_num,
                                    relationship_type=rel_type,
                                    strength=strength,
                                    context=content[:200]
                                )
                                cross_refs.append(cross_ref)
        
        return cross_refs

    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ"""
        # Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…Ø¨Ø³Ø·Ø© Ù„Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        words1 = set(re.findall(r'\w+', text1.lower()))
        words2 = set(re.findall(r'\w+', text2.lower()))
        
        # Ù…ØµØ·Ù„Ø­Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ù‡Ù…Ø© Ù„Ù‡Ø§ ÙˆØ²Ù† Ø£ÙƒØ¨Ø±
        legal_terms = {
            'Ù…ØªØ³Ø§Ø¨Ù‚', 'Ù†Ù‚Ø§Ø·', 'ÙˆÙ‚Øª', 'Ø§Ø³ØªØ¨Ø¹Ø§Ø¯', 'Ø¹Ù‚ÙˆØ¨Ø©', 'Ø«Ø§Ù†ÙŠØ©', 'Ø¯Ù‚ÙŠÙ‚Ø©',
            'contestant', 'points', 'time', 'disqualification', 'penalty', 'seconds'
        }
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        if not union:
            return 0.0
        
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ÙˆØ²Ù† Ù„Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
        weighted_intersection = len(intersection)
        for term in intersection:
            if term in legal_terms:
                weighted_intersection += 0.5
        
        return weighted_intersection / len(union)

    def _build_semantic_clusters(self, article: Dict, all_articles: List[Dict]) -> None:
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© Ù„Ù„Ù…ÙˆØ§Ø¯"""
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ (Ø§Ù„ØªÙˆÙ‚ÙŠØªØŒ Ø§Ù„Ù†Ù‚Ø§Ø·ØŒ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª...)
        content = article.get('content', '').lower()
        article_num = article.get('article_number', 'appendix')
        
        clusters = {
            'timing': ['ÙˆÙ‚Øª', 'Ø«Ø§Ù†ÙŠØ©', 'Ø¯Ù‚ÙŠÙ‚Ø©', 'Ù…Ù‡Ù„Ø©', 'time', 'second', 'minute'],
            'scoring': ['Ù†Ù‚Ø§Ø·', 'Ø¯Ø±Ø¬Ø©', 'ØªØ³Ø¬ÙŠÙ„', 'points', 'score', 'scoring'],
            'penalties': ['Ø¹Ù‚ÙˆØ¨Ø©', 'Ø§Ø³ØªØ¨Ø¹Ø§Ø¯', 'Ø®ØµÙ…', 'penalty', 'disqualification'],
            'exceptions': ['Ø§Ø³ØªØ«Ù†Ø§Ø¡', 'Ø¥Ù„Ø§', 'Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡', 'exception', 'except', 'unless']
        }
        
        for cluster_name, keywords in clusters.items():
            if any(keyword in content for keyword in keywords):
                if cluster_name not in self.semantic_clusters:
                    self.semantic_clusters[cluster_name] = []
                self.semantic_clusters[cluster_name].append(article_num)

    def perform_multi_hop_reasoning(self, question: str, context: List[Dict]) -> List[ReasoningStep]:
        """Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø§Ø­Ù„ Ù„Ø±Ø¨Ø· Ø§Ù„Ù…ÙˆØ§Ø¯"""
        reasoning_steps = []
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
        question_entities = self._extract_question_entities(question)
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
        direct_articles = self._find_direct_relevant_articles(question_entities, context)
        
        if direct_articles:
            step1 = ReasoningStep(
                step_number=1,
                reasoning=f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(direct_articles)} Ù…Ø§Ø¯Ø© Ø°Ø§Øª ØµÙ„Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„",
                evidence=f"Ø§Ù„Ù…ÙˆØ§Ø¯: {[art.get('article_number') for art in direct_articles]}",
                article_refs=[art.get('article_number') for art in direct_articles],
                confidence=0.9
            )
            reasoning_steps.append(step1)
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…ØªØ±Ø§Ø¨Ø·Ø© Ø¹Ø¨Ø± Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹Ø©
        linked_articles = self._find_cross_referenced_articles(direct_articles, context)
        
        if linked_articles:
            step2 = ReasoningStep(
                step_number=2,
                reasoning="Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙˆØ§Ø¯ Ù…ØªØ±Ø§Ø¨Ø·Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚ÙˆØ§Ù†ÙŠÙ† Ø£Ø³Ø§Ø³ÙŠØ© Ø£Ùˆ Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª",
                evidence=f"Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…ØªØ±Ø§Ø¨Ø·Ø©: {[art.get('article_number') for art in linked_articles]}",
                article_refs=[art.get('article_number') for art in linked_articles],
                confidence=0.8
            )
            reasoning_steps.append(step2)
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª ÙˆØ§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª
        exceptions_and_rules = self._analyze_rules_and_exceptions(
            direct_articles + linked_articles, question_entities
        )
        
        if exceptions_and_rules:
            step3 = ReasoningStep(
                step_number=3,
                reasoning="ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©",
                evidence=exceptions_and_rules['analysis'],
                article_refs=exceptions_and_rules['article_refs'],
                confidence=0.95
            )
            reasoning_steps.append(step3)
        
        return reasoning_steps

    def _extract_question_entities(self, question: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ù…Ù† Ø§Ù„Ø³Ø¤Ø§Ù„"""
        entities = []
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… (Ù‚Ø¯ ØªÙƒÙˆÙ† Ø£ÙˆÙ‚Ø§Øª Ø£Ùˆ Ù†Ù‚Ø§Ø·)
        numbers = re.findall(r'\d+', question)
        entities.extend(numbers)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
        legal_keywords = [
            'ØªØ£Ø®Ø±', 'Ø¹Ù‚ÙˆØ¨Ø©', 'Ø§Ø³ØªØ«Ù†Ø§Ø¡', 'Ù†Ù‚Ø§Ø·', 'Ø«Ø§Ù†ÙŠØ©', 'Ø§Ø³ØªØ¨Ø¹Ø§Ø¯',
            'delay', 'penalty', 'exception', 'points', 'seconds', 'disqualification'
        ]
        
        for keyword in legal_keywords:
            if keyword in question.lower():
                entities.append(keyword)
        
        return entities

    def _find_direct_relevant_articles(self, entities: List[str], context: List[Dict]) -> List[Dict]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©"""
        relevant_articles = []
        
        for article in context:
            content = article.get('content', '').lower()
            relevance_score = 0
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØµÙ„Ø© Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
            for entity in entities:
                if entity.lower() in content:
                    relevance_score += 1
                    
                # ÙˆØ²Ù† Ø¥Ø¶Ø§ÙÙŠ Ù„Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹
                if entity.isdigit() and f"{entity} Ø«Ø§Ù†ÙŠØ©" in content:
                    relevance_score += 2
            
            if relevance_score > 0:
                article['computed_relevance'] = relevance_score
                relevant_articles.append(article)
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØµÙ„Ø©
        return sorted(relevant_articles, key=lambda x: x.get('computed_relevance', 0), reverse=True)

    def _find_cross_referenced_articles(self, direct_articles: List[Dict], all_context: List[Dict]) -> List[Dict]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…ØªØ±Ø§Ø¨Ø·Ø© Ø¹Ø¨Ø± Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹Ø©"""
        linked_articles = []
        
        for article in direct_articles:
            article_num = article.get('article_number')
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            for cross_ref in self.cross_references:
                if cross_ref.source_article == article_num or cross_ref.target_article == article_num:
                    # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚
                    target_num = (cross_ref.target_article if cross_ref.source_article == article_num 
                                 else cross_ref.source_article)
                    
                    for context_article in all_context:
                        if context_article.get('article_number') == target_num:
                            context_article['cross_ref_type'] = cross_ref.relationship_type
                            context_article['cross_ref_strength'] = cross_ref.strength
                            linked_articles.append(context_article)
        
        return linked_articles

    def _analyze_rules_and_exceptions(self, articles: List[Dict], question_entities: List[str]) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª"""
        analysis = {
            'main_rules': [],
            'exceptions': [],
            'analysis': "",
            'article_refs': []
        }
        
        for article in articles:
            content = article.get('content', '')
            article_num = article.get('article_number')
            
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø§Ø¯Ø© (Ù‚Ø§Ù†ÙˆÙ† Ø£Ø³Ø§Ø³ÙŠ Ø£Ùˆ Ø§Ø³ØªØ«Ù†Ø§Ø¡)
            if any(word in content.lower() for word in ['Ø§Ø³ØªØ«Ù†Ø§Ø¡', 'Ø¥Ù„Ø§', 'Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡', 'exception', 'except']):
                analysis['exceptions'].append({
                    'article': article_num,
                    'content': content[:200] + "..."
                })
            else:
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§ØªØŒ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯)
                if any(word in content.lower() for word in ['Ø¹Ù‚ÙˆØ¨Ø©', 'Ø§Ø³ØªØ¨Ø¹Ø§Ø¯', 'ØµÙØ± Ù†Ù‚Ø§Ø·', 'penalty', 'disqualification']):
                    analysis['main_rules'].append({
                        'article': article_num,
                        'content': content[:200] + "..."
                    })
            
            analysis['article_refs'].append(article_num)
        
        # ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Øµ Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠ
        if analysis['main_rules'] and analysis['exceptions']:
            analysis['analysis'] = f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(analysis['main_rules'])} Ù‚Ø§Ù†ÙˆÙ† Ø£Ø³Ø§Ø³ÙŠ Ùˆ {len(analysis['exceptions'])} Ø§Ø³ØªØ«Ù†Ø§Ø¡. "
            analysis['analysis'] += "Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ØªØ­Ø¯Ø¯ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø©ØŒ ÙˆØ§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª ØªØ­Ø¯Ø¯ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ Ù„Ø§ ØªØ·Ø¨Ù‚ ÙÙŠÙ‡Ø§ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø©."
        
        return analysis

    def calculate_advanced_relevance(self, article: Dict, question: str, question_entities: List[str]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØµÙ„Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª"""
        base_score = 0.0
        content = article.get('content', '').lower()
        
        # Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„ÙƒÙŠØ§Ù†Ø§Øª
        for entity in question_entities:
            if entity.lower() in content:
                base_score += 1.0
                
                # Ù†Ù‚Ø§Ø· Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
                if entity.isdigit():
                    if f"{entity} Ø«Ø§Ù†ÙŠØ©" in content or f"{entity} second" in content:
                        base_score += 3.0
                    elif f"{entity} Ù†Ù‚Ø·Ø©" in content or f"{entity} point" in content:
                        base_score += 2.0
        
        # Ù†Ù‚Ø§Ø· Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹Ø©
        article_num = article.get('article_number')
        for cross_ref in self.cross_references:
            if cross_ref.source_article == article_num or cross_ref.target_article == article_num:
                base_score += cross_ref.strength * 2.0
        
        # Ù†Ù‚Ø§Ø· Ù„Ù„ØªØ¬Ù…ÙŠØ¹Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©
        for cluster_name, cluster_articles in self.semantic_clusters.items():
            if article_num in cluster_articles:
                # ØªØ­Ø¯ÙŠØ¯ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±Ø§Øª Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„
                cluster_relevance = {
                    'timing': 3.0 if any(word in question.lower() for word in ['ÙˆÙ‚Øª', 'Ø«Ø§Ù†ÙŠØ©', 'ØªØ£Ø®Ø±', 'time', 'delay']) else 0,
                    'penalties': 4.0 if any(word in question.lower() for word in ['Ø¹Ù‚ÙˆØ¨Ø©', 'Ø§Ø³ØªØ¨Ø¹Ø§Ø¯', 'penalty']) else 0,
                    'exceptions': 3.5 if 'Ø§Ø³ØªØ«Ù†Ø§Ø¡' in question.lower() or 'exception' in question.lower() else 0,
                    'scoring': 2.0 if 'Ù†Ù‚Ø§Ø·' in question.lower() or 'points' in question.lower() else 0
                }
                base_score += cluster_relevance.get(cluster_name, 0.5)
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¥Ù„Ù‰ Ù†Ø³Ø¨Ø© Ù…Ø¦ÙˆÙŠØ©
        max_possible_score = len(question_entities) * 4.0 + 10.0  # ØªÙ‚Ø¯ÙŠØ± Ø£Ù‚ØµÙ‰ Ù†Ù‚Ø§Ø· Ù…Ù…ÙƒÙ†Ø©
        normalized_score = min(100.0, (base_score / max_possible_score) * 100)
        
        return normalized_score

    def format_structured_response(self, question: str, reasoning_steps: List[ReasoningStep], 
                                 relevant_articles: List[Dict]) -> Dict[str, Any]:
        """ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ù‡ÙŠÙƒÙ„Ø© ÙˆØ§Ù„ÙˆØ§Ø¶Ø­Ø©"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°ÙƒÙŠØ© Ù…Ù† Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙÙƒÙŠØ±
        main_conclusion = self._generate_smart_conclusion(question, reasoning_steps, relevant_articles)
        
        # ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
        legal_references = self._organize_legal_references(relevant_articles)
        
        response = {
            "smart_legal_analysis": main_conclusion,
            "legal_foundation": legal_references,
            "reasoning_quality": {
                "steps_performed": len(reasoning_steps),
                "cross_references_found": len([step for step in reasoning_steps if "Ù…ØªØ±Ø§Ø¨Ø·Ø©" in step.reasoning]),
                "confidence_level": sum(step.confidence for step in reasoning_steps) / len(reasoning_steps) if reasoning_steps else 0.5
            }
        }
        
        return response

    def _generate_smart_conclusion(self, question: str, reasoning_steps: List[ReasoningStep], 
                                 articles: List[Dict]) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø°ÙƒÙŠØ© Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        if not reasoning_steps:
            return "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ ÙƒØ§ÙÙ Ù„Ù„Ø³Ø¤Ø§Ù„."
        
        # ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„ØªØ­Ø¯ÙŠØ¯ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        question_type = self._classify_question_type(question)
        
        conclusion = "## Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø°ÙƒÙŠ\n\n"
        
        if question_type == "penalty_timing":
            # Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª ÙˆØ§Ù„ØªÙˆÙ‚ÙŠØª
            main_rule = self._extract_main_rule(articles)
            exceptions = self._extract_exceptions(articles)
            
            if main_rule and exceptions:
                conclusion += f"**Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:** {main_rule['rule']}\n\n"
                conclusion += f"**Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª:** {exceptions['exception']}\n\n"
            elif main_rule:
                conclusion += f"**Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:** {main_rule['rule']}\n\n"
        
        elif question_type == "scoring":
            # Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ù†Ù‚Ø§Ø·
            scoring_rule = self._extract_scoring_rule(articles)
            if scoring_rule:
                conclusion += f"**Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ù‚Ø§Ø·:** {scoring_rule}\n\n"
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ
        conclusion += "**Ø§Ø³ØªÙ†Ø§Ø¯Ø§Ù‹ Ù„Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:**\n"
        
        return conclusion

    def _classify_question_type(self, question: str) -> str:
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['ØªØ£Ø®Ø±', 'Ø«Ø§Ù†ÙŠØ©', 'Ø¹Ù‚ÙˆØ¨Ø©', 'delay', 'penalty']):
            return "penalty_timing"
        elif any(word in question_lower for word in ['Ù†Ù‚Ø§Ø·', 'Ù†Ù‚Ø·Ø©', 'points', 'score']):
            return "scoring"
        elif any(word in question_lower for word in ['Ø§Ø³ØªØ«Ù†Ø§Ø¡', 'exception']):
            return "exceptions"
        else:
            return "general"

    def _extract_main_rule(self, articles: List[Dict]) -> Optional[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ"""
        for article in articles:
            content = article.get('content', '')
            if any(word in content.lower() for word in ['ØµÙØ± Ù†Ù‚Ø§Ø·', 'Ø§Ø³ØªØ¨Ø¹Ø§Ø¯', 'Ø¹Ù‚ÙˆØ¨Ø©']):
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
                rule_text = self._extract_key_sentence(content, ['ØµÙØ± Ù†Ù‚Ø§Ø·', 'Ø§Ø³ØªØ¨Ø¹Ø§Ø¯', 'Ø¹Ù‚ÙˆØ¨Ø©'])
                if rule_text:
                    return {
                        'rule': rule_text,
                        'article': article.get('article_number'),
                        'type': 'main_rule'
                    }
        return None

    def _extract_exceptions(self, articles: List[Dict]) -> Optional[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª"""
        for article in articles:
            content = article.get('content', '')
            if any(word in content.lower() for word in ['Ø§Ø³ØªØ«Ù†Ø§Ø¡', 'Ø¥Ù„Ø§', 'Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡']):
                exception_text = self._extract_key_sentence(content, ['Ø§Ø³ØªØ«Ù†Ø§Ø¡', 'Ø¥Ù„Ø§', 'Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡'])
                if exception_text:
                    return {
                        'exception': exception_text,
                        'article': article.get('article_number'),
                        'type': 'exception'
                    }
        return None

    def _extract_key_sentence(self, content: str, keywords: List[str]) -> Optional[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©"""
        sentences = content.split('.')
        for sentence in sentences:
            if any(keyword in sentence.lower() for keyword in keywords):
                return sentence.strip()
        return None

    def _extract_scoring_rule(self, articles: List[Dict]) -> Optional[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ù†Ù‚Ø§Ø·"""
        for article in articles:
            content = article.get('content', '')
            if 'Ù†Ù‚Ø§Ø·' in content.lower() or 'points' in content.lower():
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø·
                scoring_sentence = self._extract_key_sentence(content, ['Ù†Ù‚Ø§Ø·', 'Ù†Ù‚Ø·Ø©', 'points'])
                return scoring_sentence
        return None

    def _organize_legal_references(self, articles: List[Dict]) -> List[Dict]:
        """ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©"""
        references = []
        
        for article in articles:
            ref = {
                "article_number": article.get('article_number'),
                "title": article.get('title', ''),
                "key_content": article.get('content', '')[:300] + "...",
                "relevance_percentage": round(article.get('computed_relevance', 0) * 10, 1),
                "reference_type": article.get('cross_ref_type', 'direct')
            }
            references.append(ref)
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
        return sorted(references, key=lambda x: x['relevance_percentage'], reverse=True)


# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
advanced_reasoning = AdvancedLegalReasoning()